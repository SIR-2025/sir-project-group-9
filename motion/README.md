# NAO Robot Motion and Emotion Control

This project controls a NAO humanoid robot, enabling it to perform as a social robot in a theatrical setting. The robot's actions, emotions, and dialogue are dynamically generated by the DeepSeek large language model based on user-defined scenarios.

## Features

- **AI-Driven Character Performance**: The robot embodies different characters based on life stages (Child, Teen, Adult, Elderly) and psychological attachment styles (Secure, Anxious, Avoidant, Disorganized).
- **Dynamic Motion and Speech**: The system parses AI-generated responses to extract motion tags (e.g., `[angry]`, `[gesture_hey]`) and dialogue. It then triggers corresponding pre-defined animations and uses Text-to-Speech (TTS) to deliver the lines.
- **Speech-to-Text Input**: Supports voice input using Google STT, allowing for natural interaction with the robot.
- **Simulation Mode**: Can be run without a physical NAO robot for development and testing purposes.

## Project Structure

- **`main.py`**: The main application entry point. It orchestrates the interaction between the user, the DeepSeek AI, and the robot controller. It handles input (text or speech), sends it to the AI, and processes the response to command the robot.

- **`motion_controller.py`**: Contains the `EmotionMotionController` class, which serves as the bridge between the AI's commands and the robot's hardware. It maps abstract emotion/gesture tags to specific animation files and manages the robot's TTS.

- **`emo_list.py`**: Defines an extensive list of the NAO robot's built-in animations, categorized into emotions and gestures. It also provides a `MotionAnimationsApp` class for direct interaction with the robot's animation system.

- **`stt_input.py`**: A simple wrapper for Google's Speech-to-Text service, used to capture voice input from the user.

- **`nao_basic_motion.py`**: A utility script providing direct control over basic NAO robot movements like spinning and lying down. It can be run independently for testing robot functions.

## How It Works

1.  **Input**: The user provides a scenario, including a life stage and an attachment style, via the command line (either through text or voice).
2.  **AI Processing**: `main.py` sends this context to the DeepSeek API, which generates a line of dialogue prefixed with a motion tag (e.g., `[sad] What did I do wrong?`).
3.  **Control Execution**:
    - The `EmotionMotionController` parses the response, identifying the motion tag (`sad`) and the text to be spoken.
    - It looks up a suitable animation from the lists in `emo_list.py`.
    - It sends a request to the NAO robot to perform the animation and speak the dialogue using its TTS engine.
4.  **Output**: The NAO robot performs the action and speaks the line, creating an integrated character performance.

## Usage

To run the main application, you need to configure your API keys and robot IP address.

1.  Set the `DEEPSEEK_API_KEY` environment variable.
2.  Set the `NAO_IP` environment variable to your robot's IP address.
3.  (Optional) For voice input, set up Google Cloud credentials and enable the `USE_STT` flag in `main.py`.

Execute the main script:
```bash
python main.py
```