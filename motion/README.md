# NAO Robot Motion and Emotion Control (with new_main.py)

This project controls a NAO humanoid robot, enabling it to perform as a social robot in an interactive, multi-stage narrative. The robot's actions, emotions, and dialogue are dynamically generated by a large language model (currently configured for DeepSeek) based on a chosen scenario.

This document describes the flow and configuration for `new_main.py`.

## Features

- **AI-Driven Character Performance**: The robot embodies a character progressing through different life stages (`baby`, `child`, `teen`). The personality is shaped by the pre-selected "parent role" (e.g., `parent_artist`, `parent_businessman`).
- **Scenario-Based Interaction**: The simulation progresses through a series of pre-defined events for each life stage, loaded from JSON configuration files.
- **Dynamic Motion and Speech**: The system parses AI-generated responses to extract motion tags (e.g., `[angry]`, `[gesture_hey]`) and dialogue. It then triggers corresponding animations and uses Text-to-Speech (TTS) to deliver the lines.
- **Interactive Control**: The user drives the conversation turn by turn and decides when to conclude each scene.
- **Simulation Mode**: Can be run without a physical NAO robot for development and testing. All actions are printed to the console.

## Project Structure

- **`new_main.py`**: The main application entry point. It manages the overall life-stage progression, loads scenario data, handles the conversation loop with the user, sends prompts to the AI, and processes the response to command the robot.

- **`motion_controller.py`**: Contains the `EmotionMotionController` class, which serves as the bridge to the robot's hardware. It handles the connection to the NAO robot, maps emotion/gesture tags to specific animations, and manages the robot's TTS function. **Robot IP configuration is done here.**

- **`llm_prompts/`**: This directory contains the JSON files that define the robot's behavior and the scenarios.
  - `behaviour/`: Contains the structure of events for each life stage.
  - `description/`: Contains the specific scene descriptions for each parent role.

- **`emo_list.py`**: Defines an extensive list of the NAO robot's built-in animations, which `motion_controller.py` uses to find appropriate movements.

## How It Works

1.  **Initialization**: `new_main.py` starts and initializes the `EmotionMotionController`, which attempts to connect to the NAO robot. It prints whether it's connected to a real robot or running in simulation mode.
2.  **Role Selection**: The user is prompted to select a "parent role" from a list. This choice determines the context for the scenes that follow.
3.  **Life Stage Loop**: The application iterates through the `baby`, `child`, and `teen` life stages.
4.  **Scene Execution**: For each stage, the script loops through pre-defined events (e.g., "first day of school"). It constructs a system prompt for the AI using scene descriptions from the JSON files.
5.  **User Interaction**:
    - The user types their dialogue in the console.
    - The dialogue is sent to the DeepSeek API along with the conversation history.
6.  **AI Processing & Robot Action**:
    - The AI generates a response, which includes a motion tag (e.g., `[sad]`) and spoken text.
    - `new_main.py` extracts the tag and the text.
    - `motion_controller.py` is called to execute the actions:
        - `play_for_emotions()` finds and plays an animation matching the tag.
        - `speak_text()` sends the dialogue to the robot's TTS engine.
7.  **Loop Control**: After the robot's turn, the user is prompted to either continue the conversation ('0') or "wrap up" the scene ('1'), which triggers a concluding remark from the robot.

## Usage

To run the main application, you need to configure your API key and robot IP address.

1.  **API Key**: The DeepSeek API key is currently **hardcoded** inside `new_main.py` in the `create_openai_client` function. Replace the placeholder with your actual key.
2.  **Robot IP Address**: The NAO Robot's IP address is set in `motion_controller.py`. Open this file and change the default value for `nao_ip` in the `__init__` method of the `EmotionMotionController` class.
    ```python
    # In motion_controller.py
    class EmotionMotionController:
        def __init__(
            self,
            nao_ip: Optional[str] = "10.0.0.127",  # <-- CHANGE THIS IP
            # ...
        ) -> None:
    ```

3.  **Execute the main script**:
    ```bash
    python motion/new_main.py
    ```
4.  Follow the on-screen prompts to select a role and interact with the robot.

**Note**: This script does **not** use Speech-to-Text (STT) for input. All user interaction is done via the keyboard.