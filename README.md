# NAO Parenting Device: Life-Course Impact Simulation

This project turns a NAO humanoid robot into a parenting device for the SIR course. It roleplays a human life cycle to make early trauma and mistreatment visible across later life stages. The robot's motions, emotions, and dialogue are generated by an LLM and grounded in scripted scenarios.

## What It Does
- Walks through life stages (baby, child, teen) under different "parent roles" to show how early experiences shape later behavior.
- Uses NAO motions and TTS to express emotions and gestures inline with AI-generated dialogue.
- Supports console-driven interaction: you type, the robot responds with movement and speech. A simulation mode mirrors actions in the console when no robot is connected.

## Core Features
- AI-driven character performance: persona shaped by selected parent role (e.g., artist, businessman), progressing through staged events.
- Scenario-based interaction: scenes and events loaded from JSON, varying by life stage and parent role.
- Dynamic motion and speech: AI responses embed motion tags like `[sad]` or `[gesture_hey]`; the system plays matching animations and speaks the line.
- Interactive pacing: you decide when to continue or wrap each scene.
- Simulation mode: develop without a NAO; actions and speech are printed to console.

## Project Layout
- `motion/new_main.py`: Entry point; manages life-stage flow, prompt construction, user turn-taking, and parsing AI responses for motion tags.
- `motion/motion_controller.py`: Connects to NAO, maps emotion/gesture tags to animations, and sends TTS. Set the robot IP here.
- `motion/llm_prompts/`: JSON definitions for behaviors (`behaviour/`) and scene descriptions (`description/`) per parent role and stage.
- `motion/emo_list.py`: Catalog of NAO animations referenced by the controller.
- `motion/Role_description`: Role definitions that influence persona and responses.

## Prerequisites
- Python 3.8+
- Access to a NAO robot (optional in simulation mode)
- An LLM API key (currently configured for DeepSeek in code)

## Environment Setup (via conda)
1. Install Anaconda or Miniconda if not already available.
2. From the repo root, create the env: `conda env create -f environment.yml`.
3. Activate: `conda activate sic`.
4. If you update `environment.yml`, sync with: `conda env update -f environment.yml --prune`.
5. Verify inside the env: `python --version` and `python -m pip list | head` to confirm packages resolved.

## Configuration
1. **API key**: In `motion/new_main.py`, update the placeholder key inside `create_openai_client`.
2. **Robot IP**: In `motion/motion_controller.py`, set `nao_ip` in `EmotionMotionController.__init__`.

## Run
```bash
python motion/new_main.py
```
Follow the prompts to pick a parent role, then alternate turns with the robot. Enter `1` when you want to wrap up a scene; `0` to continue.

## Technical Guide (inline replacement for motion/README.md)
- Flow
  - Initialize `EmotionMotionController`; print whether NAO is connected or running simulated.
  - Prompt user to choose a parent role (shapes personality and responses).
  - Iterate life stages: baby -> child -> teen.
  - For each scene/event: load descriptions and behavior from JSON; build system prompt.
  - Conversation loop: user types a line; send history and system prompt to the LLM (DeepSeek); get response with a motion tag and spoken text.
  - Parse response: extract motion tag (e.g., `[sad]`, `[gesture_hey]`) and dialogue; trigger animation and TTS.
  - After each turn, prompt user: `0` to continue, `1` to wrap the scene (robot delivers a wrap-up line).
- Simulation mode: if NAO is unavailable, actions and speech are printed to console; no hardware needed.
- Inputs: keyboard only; there is no speech-to-text.
- Key files
  - `motion/new_main.py`: life-stage progression, prompt building, response parsing, user loop.
  - `motion/motion_controller.py`: tag-to-animation mapping and TTS; NAO IP configured in `__init__`.
  - `motion/emo_list.py`: catalog of NAO animations used by the controller.
  - `motion/llm_prompts/behaviour`: JSON event structures per life stage.
  - `motion/llm_prompts/description`: scene descriptions per parent role.
- API: uses DeepSeek via OpenAI-compatible client; the API key is currently hardcoded in `create_openai_client` (replace with your key).

## Notes for SIR Course
- Commit regularly and acknowledge contributions in commits and logbooks.
- More on SIC: https://social-ai-vu.github.io/social-interaction-cloud/index.html
